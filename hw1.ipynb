{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replicate the work we did in Hw1P3 using a neural network. \n",
    "\n",
    "We first need to install tensorflow and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the matrix A1P3 from the matlab file and store its feature vectors in trainX and its labels in trainY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "A = sio.loadmat('/Users/f002bpv/Documents/MATLAB/engg177/hw1/A1P3.mat')['A1P3']\n",
    "trainX = A[1:,:].T\n",
    "trainY = A[0,:].T.reshape(10000,1)\n",
    "#trainY = np.concatenate([trainY,abs(trainY-1)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we'll check the shapes of these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience for code reuse, we create some parameters that will inform the rest of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set Model parameters\n",
    "num_features = trainX.shape[1]\n",
    "num_labels = trainY.shape[1]\n",
    "num_samples = trainX.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set training parameters. We'll use a decaying learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "training_epochs =10000\n",
    "epoch_size = 100\n",
    "display_step = 500\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=0.01,\n",
    "                                          global_step= global_step,\n",
    "                                          decay_steps=num_samples,\n",
    "                                          decay_rate= 0.95,\n",
    "                                          staircase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set a directory that specifies where our summaries will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summaries_dir = \"/Users/f002bpv/tmp/summary_logs/\" + datetime.now().strftime('%Y%m%d_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to start coding the neural network. We'll start by opening an interactive session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open interactive Session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the placeholder variables that will eventually hold values taken from trainX and trainY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_ = tf.placeholder(tf.float32, [None, num_features])\n",
    "y_ = tf.placeholder(tf.float32, [None, num_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used to define the weight and bias variables. It will automatically create summaries of how those variables change, which we store for later visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add summary ops to collect data\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function defines a layer of a neural network. In this case, we will build a degenerate NN with only one layer, which has only one neuron. This function also generates a number of summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a layer\n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer.\n",
    "\n",
    "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "    It also sets up name scoping so that the resultant graph is easy to read,\n",
    "    and adds a number of summary ops.\n",
    "    \"\"\"\n",
    "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "    with tf.name_scope(layer_name):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = tf.Variable(\n",
    "                tf.random_normal([input_dim, output_dim], \n",
    "                                 mean=0.0, stddev=1.0, dtype=tf.float32))\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = tf.Variable(\n",
    "                tf.random_normal([output_dim], mean=0.0, stddev=1.0, \n",
    "                                 dtype=tf.float32))\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            mul_obj = tf.matmul(input_tensor, weights)\n",
    "            preactivate = tf.add(mul_obj, biases)\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = act(preactivate, name='activation')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a single neuron, using a sigmoidal activation function, which we store as F_OP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the (single-neuron) layer\n",
    "F_OP = nn_layer(x_, num_features, num_labels, 'layer1', act=tf.nn.sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given F_OP, we need a means of evaluating loss. We'll use $\\ell_2$ loss. If $\\hat{y}$ stores the outputs of the activation function for each data point in the training set, and $y$ stores their expected classes, this error is given by:\n",
    "\n",
    "$$\\frac{1}{2}\\sum\\limits_{i=1}^{10000} (\\hat{y}_i-y_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'l2_loss_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define loss\n",
    "with tf.name_scope('l2_loss'):\n",
    "    l2_loss = tf.nn.l2_loss(tf.sub(F_OP, y_))\n",
    "tf.summary.scalar('l2_loss', l2_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the activation and the loss, we need to define the learning procedure that will attempt to optimize the activation so as to minimize the loss. We'll use Gradient Descent in order to accomplish this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Optimization Procedure\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "      l2_loss, global_step = global_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to evaluate the trained neuron F_OP. In order to do so, we define a vector correct_prediction, which checks whether F_OP agrees with the labels for each training vector, and accuracy, which is the proportion of agreement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define 'Accuracy'\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.round(F_OP), y_)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to record our results for later evaluation, we combine all of the summaries and produce writer objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(summaries_dir + '/train',\n",
    "                                      sess.graph)\n",
    "test_writer = tf.summary.FileWriter(summaries_dir + '/test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're ready to go, we initialize all of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize Variables\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will now actually execute the gradient descent loop. Every so often we will evaluate the performance of the learned neuron on the total training set and print the results to screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and Training Rate at step 0: 0.343\n",
      "Accuracy and Training Rate at step 500: 0.9034\n",
      "Accuracy and Training Rate at step 1000: 0.953\n",
      "Accuracy and Training Rate at step 1500: 0.9677\n",
      "Accuracy and Training Rate at step 2000: 0.975\n",
      "Accuracy and Training Rate at step 2500: 0.9755\n",
      "Accuracy and Training Rate at step 3000: 0.982\n",
      "Accuracy and Training Rate at step 3500: 0.9813\n",
      "Accuracy and Training Rate at step 4000: 0.9867\n",
      "Accuracy and Training Rate at step 4500: 0.9891\n",
      "Accuracy and Training Rate at step 5000: 0.9886\n",
      "Accuracy and Training Rate at step 5500: 0.9895\n",
      "Accuracy and Training Rate at step 6000: 0.9924\n",
      "Accuracy and Training Rate at step 6500: 0.9881\n",
      "Accuracy and Training Rate at step 7000: 0.9882\n",
      "Accuracy and Training Rate at step 7500: 0.9907\n",
      "Accuracy and Training Rate at step 8000: 0.9896\n",
      "Accuracy and Training Rate at step 8500: 0.9893\n",
      "Accuracy and Training Rate at step 9000: 0.9927\n",
      "Accuracy and Training Rate at step 9500: 0.9907\n",
      "final cost on test set: 0.993\n"
     ]
    }
   ],
   "source": [
    "# Fit the Training Data\n",
    "for epoch in range(training_epochs):\n",
    "    if epoch % display_step == 0:  # Record summaries and test-set accuracy\n",
    "        summary, acc, rate = sess.run([merged, accuracy, learning_rate], feed_dict={x_:trainX, y_:trainY})\n",
    "        test_writer.add_summary(summary, epoch)\n",
    "        print('Accuracy and Training Rate at step %s: %s' % (epoch, acc))\n",
    "    else:  # Record train set summaries, and train\n",
    "        i = np.random.randint(0, high=num_samples, size=epoch_size)\n",
    "        summary, _ = sess.run([merged, train_step], feed_dict={x_:trainX[i,:], y_:trainY[i,:]})\n",
    "        train_writer.add_summary(summary, epoch)\n",
    "\n",
    "# Evaluate Performance\n",
    "print(\"final cost on test set: %s\" %str(sess.run(accuracy, feed_dict={x_:trainX, y_:trainY})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having finished our computation, we close the session and exit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
